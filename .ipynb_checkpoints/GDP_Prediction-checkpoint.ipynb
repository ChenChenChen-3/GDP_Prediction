{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "- [Data Preprocessing](#data-preprocessing)\n",
    "    - [Quarterly GDP-Country Date (OECD)](#gdp-oced)\n",
    "    - [Quarterly Income-Based Real GDP (BEA)](#real-gdp-bea)\n",
    "- [Merge Data](#merge)\n",
    "- [Exploratory Data Analysis](#eda)\n",
    "- [Time Series Models](#time-series-model)\n",
    "    - [AR Model](#ar)\n",
    "    - [SARIMAX](#sarimax)\n",
    "    - [Halt-Winter's Method](#halt-winter)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Resource](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing<a name=\"data-preprocessing\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Organisation for Economic Co-operation and Development(OECD) have more historical data, which can track back to 1949, but I was not sure its accuracy. For providing that, I compare the data with real gross domestic product from the Bureau of Economic Analysis data. After make sure the two data frames show the same result, I can use the real GDP from 2017-04-01 to get all the real GDP data back to 1949."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly GDP-Country Date (OECD) <a name=\"gdp-oced\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This historical quarterly real GDP data is from the Organisation for Economic Co-operation and Development. The data is the quarterly real GDP of all countries between 1949 to 2019. The following steps are my pre-process. First, I just want the USA data, so I will create a filter to help me filter out the location is not the USA. The second step, I want the time and value columns only, so I removed the rest of the column to avoid noise. The third steps, I want the time column comparable, thus; I change the type to date-time. The fourth step is to check the annual GDP. So I create a new column call annually percentage change and draw the line plots to see the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "gdp_countries = pd.read_csv('../data/quarterly_gdp_countries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of the data. The unit is Millions of current dollars.\n",
    "gdp_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Location to be USA  \n",
    "mask_usa = gdp_countries['LOCATION'] == 'USA'\n",
    "gdp = gdp_countries[mask_usa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of gdp\n",
    "gdp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of each column\n",
    "gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataframe have only Time and value\n",
    "gdp = gdp[['TIME','Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of time column to be datetime\n",
    "gdp['TIME'] = pd.to_datetime(gdp[\"TIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to percentage change.\n",
    "gdp.columns = ['time', 'pct_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp['annually \\npct_change'] = gdp['pct_change'].rolling(4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the GDP change quarterly and annually\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(gdp['time'],gdp['pct_change'], label='Quarterly GDP Percentage')\n",
    "plt.plot(gdp['time'],gdp['annually \\npct_change'], label='Annually GDP Percentage Change')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The percentage line plot show 3 significant drop of GDP over 2% in 1958, 1980, and 2009. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarterly Income-Based Real GDP (BEA) <a name='real-gdp-bea'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the quarterly industry-based gdp data.\n",
    "gdp_in = pd.read_excel('../data/real_gdp_quarterly_2017_2019.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_in.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cause I just want the GDP of each quarter to compare the percentage change,\n",
    "# so I will keep only the GDP row.\n",
    "gdp_in = gdp_in[6:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column name\n",
    "gdp_in.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the first column, because it is just Line index of industry\n",
    "gdp_in = gdp_in.drop(columns = ['Table 1.1.3. Real Gross Domestic Product, Quantity Indexes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column with row.\n",
    "gdp_int = gdp_in.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first row. \n",
    "gdp_int = gdp_int[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column. \n",
    "gdp_int.columns = ['gdp_quarterly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to indicate the time  \n",
    "gdp_int['time'] = ['2017/01/01','2017/04/01','2017/07/01','2017/10/01',\n",
    "                   '2018/01/01','2018/04/01','2018/07/01','2018/10/01',\n",
    "                   '2019/01/01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of time column to datetime\n",
    "gdp_int['time'] = pd.to_datetime(gdp_int[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column to see the percentage change and time 100% to change the unit\n",
    "gdp_int['quarterly_pct_change'] = gdp_int['gdp_quarterly'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_int.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data <a name='merge'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two gdp data frame by left to keep all data in gdp_indt\n",
    "merge_17_19 = gdp_int.merge(gdp, how = 'left', on = 'time')\n",
    "merge_17_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data by oecd data\n",
    "merge_47_19 = gdp.merge(gdp_int, how = 'left', on = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data to upside down\n",
    "merge_47_19_ud = merge_47_19.sort_values(by= 'time' ,ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index.\n",
    "merge_47_19_ud = merge_47_19_ud.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column and assign the value of gdp.\n",
    "merge_47_19_ud['gdp'] = 0\n",
    "\n",
    "# Assign the rest of the value to gdp column\n",
    "for i in range(len(merge_47_19_ud['gdp_quarterly'])):\n",
    "    if i+1 < len(merge_47_19_ud['gdp_quarterly']):\n",
    "        merge_47_19_ud['gdp'].iloc[i+1] = merge_47_19_ud['gdp_quarterly'].iloc[i] *(1 - 0.01*(merge_47_19_ud['pct_change'].iloc[i]))\n",
    "        merge_47_19_ud['gdp_quarterly'].iloc[i+1] = merge_47_19_ud['gdp'].iloc[i+1] \n",
    "    else:\n",
    "        # Assign the first value to the gdp column\n",
    "        merge_47_19_ud['gdp'] = merge_47_19_ud['gdp_quarterly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed the quarterlu_pct_change and gdp column, cause I no longer need them.\n",
    "merge_47_19_ud = merge_47_19_ud.drop(columns = ['gdp','quarterly_pct_change','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "merge_47_19_ud[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of each column.\n",
    "merge_47_19_ud.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the gdp_quarterly column to float.\n",
    "merge_47_19_ud['gdp_quarterly'] = merge_47_19_ud['gdp_quarterly'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a name='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot line plots to see the gdp trend\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.plot('time','gdp_quarterly' ,data = merge_47_19_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above figure, we can see that the real GDP growth is almost 1200% since 1947. There was one significant drop in 2009, the huge decline was because of the financial crisis which began in the United States, then influenced the entire world’s, and caused an economic recession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF & PACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to use the ARIMA model to make the prediction, thus; I ploted the autocorrelation function(ACF) and partical autocorrelation function(PACF) plots to determine the q and p values. ACF is the autocorrelation of k lags related to the values that are k intervals apart. PACF is the autocorrelation of k lags with 0th-lag directly, which is not affected from 1st-lag to (k-1)th-lag. From the slowly decaying ACF plot, I can see that the future values of GDP are heavily affected by past value. With the geometric ACF and significat first lag of PACF, I can use both AR(1) model and ARIMA(1,0,0) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the time and\n",
    "df_merge = merge_47_19_ud[['time','gdp_quarterly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the data frame upside down\n",
    "df = df_merge.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of df \n",
    "df = df.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want the lags to be equal to the year\n",
    "2019-1947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the autocorrelation for each quarter\n",
    "df['gdp_quarterly'].autocorr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the autocorrelation for each year\n",
    "df['gdp_quarterly'].autocorr(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the acf and pacf ploting packages\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF plot\n",
    "plot_acf(df, lags = 72 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ACF plot above shows the good positive correlation with the lags upto 27, this is the point where the lag cuts into confidence threshold. Although we have good correlation upto 27th lag, we cannot use all of them as it will create multi-collinearity problem, thats why we turn to PACF plot to get only the most relevant lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot\n",
    "plot_pacf(df,lags=72);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above plot we can see that only the first lag have good correlation before the plot first cuts the upper confidence interval. This is our p value i.e the order of our AR process. We can use AR(1) or ARIMA(1,0,0) model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the seasonal_decompose package\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposed = seasonal_decompose(df['gdp_quarterly'], model='additive')\n",
    "decomposed.plot() \n",
    "plt.xlabel('Year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the seasonality information extracted from the series does seem reasonable. The residuals are also interesting, showing periods of high variability in the early and later years of the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models<a nema='time-series-model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are servel time series model, and the most common one is ARIMA. Before using the ARIMA model, I need to know is the data stationary or not. The (simplified) definition of a stationary process is that the mean and variance of the process are constant over time. Two methods can be use to test stationary. The first test is to see the trend of rolling mean and rolling standard deviation. I set the rolling window to be equal to 4 so I can get the annual data. If the mean and standard deviation shown as a constant, then the data is stationary. On the other hands, it's not stationary. The second test is the Dicky-Fuller Test, I can import the package to run the test. The results showed the test statistic, p-value, number of lags used, number of observation used, critical value of 1%, 5%, and 10%. \n",
    "\n",
    "I already know the p and q value from the ACF and PACF. Then, from the decompose plot, I know that the data can be seasonal and stationary. However, because of the limitations of ARIMA when it comes to seasonal data, I will use SARIMAX model instead. The SARIMAX is the extension of ARIMA that explicitly models the seasonal element in univariate data.\n",
    "\n",
    "I also considered using Simple Exponential Smoothing (SES) and Holt-Winters’ Method. SES is reweighted model which weighted the latest data more heavy and the oldest data lightly. It is a good choice for forecasting data with no clear trend or seasonal pattern. But the GDP data had seasonal pattern, I won't use SES. Holt-Winters’ Method is suitable for data with trends and seasonalities which includes a seasonality smoothing parameter γ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standard Deviation Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line plot for rolling mean and std to see is the data stationary or not.\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df.index,df,\n",
    "         label = 'Quarterly GDP')\n",
    "plt.plot(df.index,df.rolling(4).mean(),\n",
    "         label = 'Annually GDP')\n",
    "plt.plot(df.index,df.rolling(4).std(),\n",
    "         label = 'Annually GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the above line charts, we can see that the mean change over time, which means that the process is non-stationary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicky-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dicky-Fuller package\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def dicky_fuller(dataframe):\n",
    "    # Use Dicky-Fuller Test to see if the data is stationary\n",
    "    print('Result of Dicky-Fuller Test')\n",
    "    dicky = adfuller(dataframe['gdp_quarterly'])\n",
    "\n",
    "    # Print the result \n",
    "    print('Test Statistic: ',dicky[0])\n",
    "    print('p-value: ',dicky[1])\n",
    "    print('No. of Lags: ',dicky[2])\n",
    "    print('No. of Observations: ',dicky[3])\n",
    "    print('Critical Value(1%): ',dicky[4]['1%'])\n",
    "    print('Critical Value(5%): ',dicky[4]['5%'])\n",
    "    print('Critical Value(10%): ',dicky[4]['10%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicky_fuller(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The p-value is 0.02278 which is larger then 0.005. That is to say, we cannot reject the null hypothesis, which assumed the data is stationary, the data is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR(1) Model <a name='ar'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 80% of the data as training set, and the latest 20% data as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "from statsmodels.tsa.arima_model import AR\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the number of 80% of the data\n",
    "0.8*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train and test of data\n",
    "train =  df['gdp_quarterly'][:230]\n",
    "test = df['gdp_quarterly'][230:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate model \n",
    "ar_model = AR(train)\n",
    "\n",
    "# Fit model\n",
    "ar = ar_model.fit(maxlag=1)\n",
    "\n",
    "# make predictions \n",
    "ar_train_pred = ar.predict(start= ar.k_ar, \n",
    "                            end=len(train))\n",
    "ar_test_pred = ar.predict(start=len(train), \n",
    "                          end=(len(train) + len(test)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(traindata, testdata, train_pred, test_pred):\n",
    "    # Test the accuracy of train and test data\n",
    "    print('Train MSE score:', mean_squared_error(traindata,\n",
    "                                                 train_pred))\n",
    "    print('Test MSE score:', mean_squared_error(testdata,\n",
    "                                                test_pred))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(train, test, ar_train_pred, ar_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train MSE score is much smaller than the test MSE score which shows the AR(1) model is overfit.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1 = 'Data Split 80% for Training & 20% for Testing'\n",
    "split_2 = 'Data Split at 2009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_line_plot( model, traindata,\n",
    "                      testdata,train_pred,\n",
    "                      test_pred, split_type):\n",
    "    # Plot Training Predict\n",
    "    feature = ['Historical Data','Predicted Data']\n",
    "\n",
    "    plt.figure(figsize =(8,4))\n",
    "    plt.plot(traindata.index, traindata)\n",
    "    plt.plot(traindata.index, train_pred)\n",
    "    # Set labels.\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('GDP(Million Dollars)')\n",
    "    title ='Predicted v.s. Historical GDP(Training)-'+model+'\\n'+split_type\n",
    "    plt.title(title)\n",
    "    plt.legend(labels = feature)\n",
    "    \n",
    "    # Plot Testing Predict\n",
    "    plt.figure(figsize =(8,4))\n",
    "    plt.plot(testdata.index, testdata)\n",
    "    plt.plot(testdata.index, test_pred)\n",
    "    # Set labels.\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('GDP(Million Dollars)')\n",
    "    title ='Predicted v.s. Historical GDP(Testing)-'+model+'\\n'+split_type\n",
    "    plt.title(title)\n",
    "    plt.legend(labels = feature)\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict_line_plot(\"AR\", train, test, ar_train_pred,\n",
    "                  ar_test_pred, split_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above plot shows that the prediction has a significant change start from 2008. The drop in 2009 is the biggest economic crisis around the world in the 21st century. The model doesn’t predict the drop successfully, which causes the overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected year 2009 as the critical point to split the data and built models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the plot above, my theory was the trend of GDP remaining the same if we skip the Great Recession year. Thus, I selected the first quarter of 2009 as the start point of the testing set. The result of new data is totally different compared with the old one. It made the model became much more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select the year 2009 to split  \n",
    "df['2009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train and test of data\n",
    "train_2 =  df['gdp_quarterly'][:'2009-10-01']\n",
    "test_2 = df['gdp_quarterly']['2010-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of train and test\n",
    "print('Train shape' , train_2.shape)\n",
    "print('Test shape' , test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate model \n",
    "ar_model = AR(train_2)\n",
    "\n",
    "# Fit model\n",
    "ar = ar_model.fit(maxlag=1)\n",
    "\n",
    "# make predictions \n",
    "train_2_predictions = ar.predict(start= ar.k_ar, \n",
    "                              end=len(train_2))\n",
    "test_2_predictions = ar.predict(start=len(train_2), \n",
    "                              end=len(train_2) + len(test_2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(train_2, test_2, train_2_predictions, test_2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train MSE score was smaller than the test MSE score which indicates the AR(1) model was still overfit. However, the result was better than split the train and test data 80%/20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot(train_2, 'AR','Training', train_2_predictions,\n",
    "          'Data Split at 2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_plot(test_2, 'AR','Testing', test_2_predictions,\n",
    "          'Data Split at 2009')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX Model <a name='sarimax'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best d value\n",
    "for d in range(1, len(df['gdp_quarterly'])):\n",
    "    \n",
    "    # Print out a counter and the corresponding p-value.\n",
    "    print(f'Checking difference of {d}.')\n",
    "    print(f'p-value = {adfuller(df[\"gdp_quarterly\"].diff(d).dropna())[1]}')\n",
    "      \n",
    "    # Check if p < alpha.\n",
    "    if adfuller(df['gdp_quarterly'].diff(d).dropna())[1] < 0.05:\n",
    "        print(f'Differencing our time series by d={d} yields a stationary time series.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def sarimax(traindata,testdata):\n",
    "    # Initiate the model\n",
    "    sarimax_model = SARIMAX(train, order =(1,1,0),\n",
    "                            seasonal_order=(1,1,0,1))\n",
    "\n",
    "    # Fit the model\n",
    "    sarimax = sarimax_model.fit()\n",
    "\n",
    "    # Make prediction \n",
    "    sarimax_train_pred = sarimax.predict(start= 1, end=len(train))\n",
    "\n",
    "    sarimax_test_pred = sarimax.predict(start= len(train), end=287)\n",
    "    \n",
    "    # Get the MSE score\n",
    "    print('train MSE score:', mean_squared_error(train, sarimax_train_pred))\n",
    "    print('test MSE score:', mean_squared_error(test, sarimax_test_pred) )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SARIMAX model shows much more overfit MSE score of test set compare with the AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot(train, 'SARIMAX','Training', sarimax(train, test),\n",
    "          'Data Split at 2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set predict and historical data compare\n",
    "feature = ['Historical Data','Predicted Data']\n",
    "\n",
    "plt.figure(figsize =(8,4))\n",
    "plt.plot(train.index, train)\n",
    "plt.plot(train.index, sarimax_train_pred)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('GDP(Million Dollars)')\n",
    "plt.title('SARIMAX-Predicted v.s. Historical GDP(Training)')\n",
    "plt.legend(labels = feature);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set predict and historical data compare\n",
    "feature = ['Historical Data','Predicted Data']\n",
    "\n",
    "plt.figure(figsize =(8,4))\n",
    "plt.plot(test.index, test)\n",
    "plt.plot(test.index, sarimax_test_pred)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('GDP(Million Dollars)')\n",
    "plt.title('SARIMAX-Predicted v.s. Historical GDP(Testing)')\n",
    "plt.legend(labels = feature);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halt-Winter's Method<a name='halt-winter'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import exponential smoothing\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, Holt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the model\n",
    "exp_add_model = ExponentialSmoothing(df, seasonal_periods=4,\n",
    "                                     trend='add', seasonal='add')\n",
    "# Fit the model\n",
    "exp_add = exp_add_model.fit(use_boxcox=True)\n",
    "\n",
    "# Make prediction\n",
    "exp_add_trian_pred = exp_add.predict(start = 1,\n",
    "                                     end = len(train))\n",
    "exp_add_test_pred = exp_add.predict(start = len(train),\n",
    "                                    end = len(train)+len(test)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE score \n",
    "print('Train MSE score', mean_squared_error(train, exp_add_trian_pred))\n",
    "print('Test MSE score', mean_squared_error(test, exp_add_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a name='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, since 2009, except for 2009, the growth trend of GDP and total imports has increased. The huge decline in 2009 was due to the financial crisis that began in the United States and the global economic recession that caused the world's economic recession. Export trends fell twice in 2009 and 2015 to 2016. For the year 2015 to 2016, the weak global economy and the strengthening of the US dollar, exports have fallen, making US goods and services more expensive. Second, the ridge model with five features, which include 'trade_balance', 'year', 'inv', 'population', and 'employee', worked the best. It showed 99 percent of the GDP can be explained by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvement <a name='future-improve'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GDP is so correlated to import, I don't why for now. But I do need to keep learning more about their relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource <a name='resource'> </a>\n",
    "\n",
    "- GDP: \n",
    "    - BEA: https://apps.bea.gov/iTable/iTable.cfm?reqid=19&step=2#reqid=19&step=2&isuri=1&1921=survey\n",
    "    - OECD: https://stats.oecd.org/index.aspx?queryid=350\n",
    "- Economic drop: \n",
    "    - https://www.marketwatch.com/story/us-exports-fall-in-2015-for-first-time-since-recession-2016-02-05\n",
    "    - https://www.forbes.com/2009/01/14/global-recession-2009-oped-cx_nr_0115roubini.html#49d387e5185f\n",
    "- Model:\n",
    "    - https://www.youtube.com/watch?v=e8Yw4alG16Q\n",
    "    - https://www.biaodianfu.com/acf-pacf.html\n",
    "    - https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/\n",
    "    - https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/\n",
    "    - https://medium.com/datadriveninvestor/how-to-build-exponential-smoothing-models-using-python-simple-exponential-smoothing-holt-and-da371189e1a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
